{"posts":[{"title":"IPv4分类网络、CIDR、子网和专有网络介绍","text":"IPv4分类网络背景IPv4地址起初是用8位网络地址+24位主机地址来表示，这导致了独立网络的数量不能太多（最多254个），于是出现了分类网络。 介绍在分类网络下，IP地址被分为5类： Class 前缀位 网络地址位数 剩余的位数 网络数 每个网络的主机数 A类地址 0 8 24 128 16,777,214 B类地址 10 16 16 16,384 65,534 C类地址 110 24 8 2,097,152 254 D类地址（群播） 1110 未定义 未定义 未定义 未定义 E类地址（保留） 1111 未定义 未定义 未定义 未定义 此时的网络没有子网、掩码的概念，因为掩码可以从IP地址推出，所有的网络设备都会通过查看IP地址的前几位来确定地址所属的类别。 结局鉴于IPv4仍然在不断变得短缺，主要问题是：多数的网站对c类的网络地址来说太大了，因此它们都得到了b类的地址。分类网络于1993年被无类别域间路由（CIDR）取代以解决这个问题。 无类别域间路由（CIDR）CIDR是一个用于给用户分配IP地址以及在互联网上有效地路由IP数据包的对IP地址进行归类的方法。 引入子网意义 使单个站点能够构建多个局域网 将接收IP广播的区域划分成了若干部分，减少了因特网路由表中的表项数量，减少了网络开销 充分利用IP地址空间，缓解了IPv4地址紧缺的现象 区块起初子网是按分类网络来划分区块的，后来CIDR的出现，使得子网掩码并不局限于整数个八位位组的情况，任意长度的相同前缀都可以成为一个子网。 子网掩码两种表示方式 4个十进制数的格式：255.255.255.0，表示前24位是子网地址，后8位为主机地址 CIDR形式：192.168.0.1/24，/24表示前24位是子网地址，后8位为主机地址 专有网络简介专有网络是从IP地址中划分出的一块特殊区域，仅限于构建局域网或内部网络时使用。 以下是具体的划分 RFC1918 规定区块名 IP地址区段 IP数量 分类网络说明 最大CIDR区块 （子网掩码） 主机端位长 24位区块 10.0.0.0 – 10.255.255.255 16,777,216 单个A类网络 10.0.0.0/8 (255.0.0.0) 24位 20位区块 172.16.0.0 – 172.31.255.255 1,048,576 16个连续B类网络 172.16.0.0/12 (255.240.0.0) 20位 16位区块 192.168.0.0 – 192.168.255.255 65,536 256个连续C类网络 192.168.0.0/16 (255.255.0.0) 16位 意义 私有IP无法直接被互联网所访问，因此，相对于公网IP地址，它更加安全。 私有IP常被用于家庭，学校和企业的局域网。 通过局域网将域内设备互联，统一上网出入口，节约了IP地址空间，常见路由器。 疑惑 尽管从技术角度来说子网0是无效的，但它仍是可用的。例如以255.255.0.0为子网掩码的子网：1.0.0.0。这个子网有一个问题，就是它对应的单播地址和该A类网络整体对应的单播地址是一样的。 255.254.0.0（或“/15”）同样是一个有效的掩码。如果将它应用到A类地址上，就会产生128个间隔为2的子网（例如1.2.0.1～1.3.255.254，1.4.0.1～1.5.255.254等等）。这情况我们可以说是 Class A 的 network 向 host 借了 7个bits，也可以说是 host 向 Class B 的 network 借了 1个bits，后者又称为超级线路网。","link":"/2019/12/28/IPv4%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C%E3%80%81CIDR%E3%80%81%E5%AD%90%E7%BD%91%E5%92%8C%E4%B8%93%E6%9C%89%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/"},{"title":"SSH隧道实现","text":"场景在外面要连接自己家里的电脑（Linux/MacOS），实现内网穿透 设备 一台能上网的Linux/MacOS主机 一台拥有公网IP的服务器 命令 ssh -L 本地端口转发，访问本地的某个端口如同访问服务器的端口 ssh -R 远程端口转发，访问服务器的某个端口如同访问本地的端口 ssh -D 动态端口转发，用于代理 步骤 在服务器上设置sshd允许监听所有IP 12sudo echo &quot;GatewayPorts yes&quot; &gt;&gt; /etc/ssh/sshd_confgsudo systemctl restart sshd 一步到位：在本地主机上设置远程端口转发。服务器开始监听2222端口，访问服务器2222端口如同访问本地主机22端口 1ssh -R 2222:localhost:22 user@server -N 在任意可上网主机上连接以上本地主机。其中user对应本地主机的用户名，server是服务器地址。连接成功即OK。 1ssh user@server -p 2222 使用本地主机上网在任意可上网主机执行如下命令即可连接到本地主机，并将当前主机的2222端口设置为socks5代理端口。 1ssh -D 3333 user@server -p 2222 -N 使用chrome浏览器的SwitchyOmega插件可以设置socks5代理，将代理设置为localhost:3333即可使用以上本地主机的网络环境上网，同样可以直接访问所有在本地主机网域内的其他主机。 可能遇到的问题Connection refused 本地主机未开启sshd服务，若是Linux，直接运行 1sudo systemctl start sshd 即可启动ssh服务若是MacOS，请自行查阅相关资料 本地主机sshd服务未监听0.0.0.0（所有IP），需要修改配置文件/etc/sshd_config 12sudo echo &quot;ListenAddress 0.0.0.0&quot; &gt;&gt; /etc/ssh/sshd_confgsudo systemctl restart sshd","link":"/2019/09/26/SSH%E9%9A%A7%E9%81%93%E5%AE%9E%E7%8E%B0/"},{"title":"Windows下禁用笔记本电脑自带键盘","text":"1.用管理员身份打开cmd命令提示符，运行 1sc config i8042prt start=disabled 2.重新启动 3.重新开启时运行 1sc config i8042prt start=auto","link":"/2020/03/23/Windows%E4%B8%8B%E7%A6%81%E7%94%A8%E7%AC%94%E8%AE%B0%E6%9C%AC%E7%94%B5%E8%84%91%E8%87%AA%E5%B8%A6%E9%94%AE%E7%9B%98/"},{"title":"【转载】可伸缩性&#x2F;可扩展性(Scalable&#x2F;scalability)","text":"转载自 https://www.jdon.com/scalable.html 可伸缩性/可扩展性(Scalable/scalability)可伸缩性(可扩展性)是一种对软件系统计算处理能力的设计指标，高可伸缩性代表一种弹性，在系统扩展成长过程中，软件能够保证旺盛的生命力，通过很少的改动甚至只是硬件设备的添置，就能实现整个系统处理能力的线性增长，实现高吞吐量和低延迟高性能。 可伸缩性和纯粹性能调优有本质区别， 可伸缩性是高性能、低成本和可维护性等诸多因素的综合考量和平衡，可伸缩性讲究平滑线性的性能提升，更侧重于系统的水平伸缩，通过廉价的服务器实现分布式计算；而普通性能优化只是单台机器的性能指标优化。他们共同点都是根据应用系统特点在吞吐量和延迟之间进行一个侧重选择，当然水平伸缩分区后会带来CAP定理约束。 软件的可扩展性设计非常重要，但又比较难以掌握，业界试图通过云计算或高并发语言等方式节省开发者精力，但是，无论采取什么技术，如果应用系统内部是铁板一块，例如严重依赖数据库，系统达到一定访问规模，负载都集中到一两台数据库服务器上，这时进行分区扩展伸缩就比较困难，正如Hibernate框架创建人Gavin King所说：关系数据库是最不可扩展的。 性能和扩展性 什么是性能问题？如果你的系统对于一个用户访问还很慢，那就是性能问题； 什么是扩展性问题？如果你的系统对一个用户来说是快的，但是在用户不断增长的高访问量下就慢了。 延迟和吞吐量延迟和吞吐量是衡量可扩展性的一对指标，我们希望获得低延迟和高吞吐量的系统架构。所谓低延迟，也就是用户能感受到的系统响应时间，比如一个网页在几秒内打开，越短表示延迟越低，而吞吐量表示同时有多少用户能够享受到这种低延迟，如果并发用户量很大时，用户感觉网页的打开速度很慢，这意味着系统架构的吞吐量有待提高。 扩展性的目标是用可接受的延迟获得最大的吞吐量。可靠性(可用性)目标：用可接受的延迟获得数据更新的一致性。","link":"/2020/03/22/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E5%8F%AF%E4%BC%B8%E7%BC%A9%E6%80%A7-%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7-Scalable-scalability/"},{"title":"便捷你的SSH","text":"便捷你的SSH当有两台或以上云服务器的时候，每次登录都要 12$ ssh &lt;user&gt;@&lt;hostname&gt; [-p ...]password: ****** 这样会造成很多不必要的麻烦——每次都要输入用户和密码，于是我们通过配置ssh-key来认证服务器。 使用ssh-keygen生成ssh-key密钥对1$ ssh-keygen 使用ssh-copy-idssh-copy-id可以将上面生成的ssh-key复制到服务器，具体用法如下 12$ ssh-copy-id -i &lt;identity_file&gt; &lt;user&gt;@&lt;hostname&gt;password: ****** 在执行完成后就可以直接访问服务器而免去了输入密码的烦恼。 使用ssh-config虽然免去了输入密码的烦恼，但我们每次还是要输入user和hostname，我们可以通过配置一个config文件来达到更简单的目的。 编辑文件 1$ vim ~/.ssh/config 1234Host aliyun HostName ***.***.***.*** IdentityFile ~/.ssh/id_rsa user root 其中Host是别名，HostName是服务器地址，IdentityFile是私钥文件，user是要登录的用户。于是只需要 1$ ssh aliyun 就可以直接连接到服务器了。 github使用多个私钥虽然推荐一个用户只拥有一对密钥，但有时也会出现一台机器上有多个github用户的时候，如果不配置，默认就是用~/.ssh/id_rsa这个私钥，如果不区分，就可能没有权限push到指定的仓库。 比如，现在有2个github用户，每个用户有一个仓库，并且有两对ssh-key，两把ssh-key的公钥都已经放在了github相应的账户中。 编辑config文件 1$ vim ~/.ssh/config 123456789Host user1 HostName github.com IdentityFile ~/.ssh/key1 user user1Host user2 HostName github.com IdentityFile ~/.ssh/key2 user user2 此时的Host任意命名即可，看到，虽然我们配置了两个HostName都是github.com，但是它们对应不同的user。 使用时，在github的remote url中，指定相应用户即可。 例如 1git@github.com:user1/project1","link":"/2019/06/13/%E4%BE%BF%E6%8D%B7%E4%BD%A0%E7%9A%84SSH/"},{"title":"别回头，向前看","text":"","link":"/2023/07/10/%E5%88%AB%E5%9B%9E%E5%A4%B4%EF%BC%8C%E5%90%91%E5%89%8D%E7%9C%8B/"},{"title":"活着的三个理由","text":"活着的三个理由 追求爱情 追求知识 同情心 我在想旅行、探索世界这种应该属于“追求知识”这块 但和朋友一起打游戏也让我很快乐，算哪个呢？我想我应该有其他理由：娱乐和友情 我应该好好活着，期待接下来的人生是精彩的； 可是娱乐和友情似乎也并不是必要的，而追求知识有没有经济支持；我已经失去爱情了，准确的说是已经没有信心再遇到了；我只剩下同情心了。 同情心和身心痛苦在不断地斗争，这场战争结束后，我也该结束了。","link":"/2023/10/15/%E6%B4%BB%E7%9D%80%E7%9A%84%E4%B8%89%E4%B8%AA%E7%90%86%E7%94%B1/"},{"title":"用iptables中转Shadowsocks","text":"以下操作在中转服务器上进行 启用iptables转发1vi /etc/sysctl.conf 将 net.ipv4.ip_forward=0 ，修改成 net.ipv4.ip_forward=1 编辑完后使用以下命令让配置立即生效 1sysctl -p 添加iptables规则1234iptables -t nat -A PREROUTING -p tcp --dport [端口号] -j DNAT --to-destination [目标IP]iptables -t nat -A PREROUTING -p udp --dport [端口号] -j DNAT --to-destination [目标IP]iptables -t nat -A POSTROUTING -p tcp -d [目标IP] --dport [端口号] -j SNAT --to-source [本地服务器公网IP]iptables -t nat -A POSTROUTING -p udp -d [目标IP] --dport [端口号] -j SNAT --to-source [本地服务器公网IP] 此时，中转服务器上的端口号和目标IP的端口号绑定在一起了，请求中转服务器端口如同请求Shadowsocks服务器的端口","link":"/2019/07/02/%E7%94%A8iptables%E4%B8%AD%E8%BD%ACShadowsocks/"},{"title":"简单使用kubeadm构建K8S集群","text":"环境 主机：macOS Catalina 10.15 u1：ubuntu 18.04 (parallel 2GiB) u2：ubuntu 18.04 (parallel 2GiB, master node) 准备 container runtime (Docker/containerd/CRI-O) 这里使用的是Docker，Docker安装说明网上已有教程 kubeadm、kubelet、kubectl，国内请使用代理，否则无法访问google.com 12345678sudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https curlcurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -cat &lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.listdeb https://apt.kubernetes.io/ kubernetes-xenial mainEOFsudo apt-get updatesudo apt-get install -y kubelet kubeadm kubectlsudo apt-mark hold kubelet kubeadm kubectl 开始 获取所需要的镜像（虚拟机u1、u2） 12345678➜ ~ image kubeadm config images listk8s.gcr.io/kube-apiserver:v1.17.4k8s.gcr.io/kube-controller-manager:v1.17.4k8s.gcr.io/kube-scheduler:v1.17.4k8s.gcr.io/kube-proxy:v1.17.4k8s.gcr.io/pause:3.1k8s.gcr.io/etcd:3.4.3-0k8s.gcr.io/coredns:1.6.5 将如上镜像地址并保存到images.txt文件中，运行如下脚本拉取镜像（这是由于国内无法访问k8s.gcr.io而需要将其换做国内源） 12345678910images=$(cat images.txt)for image in $images;do tmp=$(echo $image | sed &quot;s#k8s.gcr.io#registry.cn-hangzhou.aliyuncs.com/google_containers#g&quot; | sed &quot;s#calico#quay.azk8s.cn/calico#g&quot;) docker pull $tmp docker tag $tmp $(echo $image | sed &quot;s#registry.cn-hangzhou.aliyuncs.com/google_containers#k8s.gcr.io#g&quot; | sed &quot;s#quay.azk8s.cn/calico#calico#g&quot;) docker rmi $tmpdone; 执行kubeadm init（虚拟机u2） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566➜ ~ kubeadm init[init] Using Kubernetes version: vX.Y.Z[preflight] Running pre-flight checks[preflight] Pulling images required for setting up a Kubernetes cluster[preflight] This might take a minute or two, depending on the speed of your internet connection[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;[kubelet-start] Activating the kubelet service[certs] Using certificateDir folder &quot;/etc/kubernetes/pki&quot;[certs] Generating &quot;etcd/ca&quot; certificate and key[certs] Generating &quot;etcd/server&quot; certificate and key[certs] etcd/server serving cert is signed for DNS names [kubeadm-cp localhost] and IPs [10.138.0.4 127.0.0.1 ::1][certs] Generating &quot;etcd/healthcheck-client&quot; certificate and key[certs] Generating &quot;etcd/peer&quot; certificate and key[certs] etcd/peer serving cert is signed for DNS names [kubeadm-cp localhost] and IPs [10.138.0.4 127.0.0.1 ::1][certs] Generating &quot;apiserver-etcd-client&quot; certificate and key[certs] Generating &quot;ca&quot; certificate and key[certs] Generating &quot;apiserver&quot; certificate and key[certs] apiserver serving cert is signed for DNS names [kubeadm-cp kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.138.0.4][certs] Generating &quot;apiserver-kubelet-client&quot; certificate and key[certs] Generating &quot;front-proxy-ca&quot; certificate and key[certs] Generating &quot;front-proxy-client&quot; certificate and key[certs] Generating &quot;sa&quot; key and public key[kubeconfig] Using kubeconfig folder &quot;/etc/kubernetes&quot;[kubeconfig] Writing &quot;admin.conf&quot; kubeconfig file[kubeconfig] Writing &quot;kubelet.conf&quot; kubeconfig file[kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file[kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file[control-plane] Using manifest folder &quot;/etc/kubernetes/manifests&quot;[control-plane] Creating static Pod manifest for &quot;kube-apiserver&quot;[control-plane] Creating static Pod manifest for &quot;kube-controller-manager&quot;[control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot;[etcd] Creating static Pod manifest for local etcd in &quot;/etc/kubernetes/manifests&quot;[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &quot;/etc/kubernetes/manifests&quot;. This can take up to 4m0s[apiclient] All control plane components are healthy after 31.501735 seconds[uploadconfig] storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace[kubelet] Creating a ConfigMap &quot;kubelet-config-X.Y&quot; in namespace kube-system with the configuration for the kubelets in the cluster[patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;kubeadm-cp&quot; as an annotation[mark-control-plane] Marking the node kubeadm-cp as control-plane by adding the label &quot;node-role.kubernetes.io/master=''&quot;[mark-control-plane] Marking the node kubeadm-cp as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule][bootstrap-token] Using token: &lt;token&gt;[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[bootstraptoken] creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace[addons] Applied essential addon: CoreDNS[addons] Applied essential addon: kube-proxyYour Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a Pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: /docs/concepts/cluster-administration/addons/You can now join any number of machines by running the following on each nodeas root: kubeadm join &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --token &lt;token&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt; 打印内容中 kubeadm join &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --token &lt;token&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;需要记录一下，token和discovery-token-ca-cert-hash用于将新的节点加入到集群 根据说明，在自己的用户目录下建立.kube文件夹并将k8s config文件拷贝过去（虚拟机u2） 123mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 测试kubectl是否正常（虚拟机u2） 123➜ ~ kubectl get nodesNAME STATUS ROLES AGE VERSIONu2 Ready master 39h v1.17.4 安装网络插件（版本可能已更新，可参考官方文档https://docs.projectcalico.org/getting-started/kubernetes/quickstart）（虚拟机u2） 12345# images_.txtcalico/pod2daemon-flexvol:v3.13.1calico/node:v3.13.1calico/cni:v3.13.1calico/kube-controllers:v3.13.1 同样适用国内源进行下载 123456789images=$(cat images_calico.txt)for image in $images;do tmp=$(echo $image | sed &quot;s#k8s.gcr.io#registry.cn-hangzhou.aliyuncs.com/google_containers#g&quot; | sed &quot;s#calico#quay.azk8s.cn/calico#g&quot;) docker pull $tmp docker tag $tmp $(echo $image | sed &quot;s#registry.cn-hangzhou.aliyuncs.com/google_containers#k8s.gcr.io#g&quot; | sed &quot;s#quay.azk8s.cn/calico#calico#g&quot;) docker rmi $tmpdone; 加载到k8s中 1kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml 加入新的节点u1（切换到虚拟机u1） 将上面的kubeadm join命令复制下来执行，⚠️注意填写正确的u2地址和端口（一般为6443） 1kubeadm join &lt;u2_IP&gt;:6443 --token &lt;token&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;```","link":"/2020/03/17/%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8kubeadm%E6%9E%84%E5%BB%BAK8S%E9%9B%86%E7%BE%A4/"},{"title":"【转载】Linux IO 模式及 select、poll、epoll 详解","text":"【转载】原文地址 https://segmentfault.com/a/1190000003063859 注：本文是对众多博客的学习和总结，可能存在理解错误。请带着怀疑的眼光，同时如果有错误希望能指出。 同步 IO 和异步 IO，阻塞 IO 和非阻塞 IO 分别是什么，到底有什么区别？不同的人在不同的上下文下给出的答案是不同的。所以先限定一下本文的上下文。 1本文讨论的背景是Linux环境下的network IO。 在进行解释之前，首先要说明几个概念： 用户空间和内核空间 进程切换 进程的阻塞 文件描述符 缓存 I/O 用户空间与内核空间现在操作系统都是采用虚拟存储器，那么对 32 位操作系统而言，它的寻址空间（虚拟存储空间）为 4G（2 的 32 次方）。操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核（kernel），保证内核的安全，操心系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。针对 linux 操作系统而言，将最高的 1G 字节（从虚拟地址 0xC0000000 到 0xFFFFFFFF），供内核使用，称为内核空间，而将较低的 3G 字节（从虚拟地址 0x00000000 到 0xBFFFFFFF），供各个进程使用，称为用户空间。 进程切换为了控制进程的执行，内核必须有能力挂起正在 CPU 上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换。因此可以说，任何进程都是在操作系统内核的支持下运行的，是与内核紧密相关的。 从一个进程的运行转到另一个进程上运行，这个过程中经过下面这些变化： 保存处理机上下文，包括程序计数器和其他寄存器。 更新 PCB 信息。 把进程的 PCB 移入相应的队列，如就绪、在某事件阻塞等队列。 选择另一个进程执行，并更新其 PCB。 更新内存管理的数据结构。 恢复处理机上下文。 注：总而言之就是很耗资源，具体的可以参考这篇文章：进程切换 进程的阻塞正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作做等，则由系统自动执行阻塞原语 (Block)，使自己由运行状态变为阻塞状态。可见，进程的阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程（获得 CPU），才可能将其转为阻塞状态。当进程进入阻塞状态，是不占用CPU资源的。 文件描述符 fd文件描述符（File descriptor）是计算机科学中的一个术语，是一个用于表述指向文件的引用的抽象化概念。 文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于 UNIX、Linux 这样的操作系统。 缓存 I/O缓存 I/O 又被称作标准 I/O，大多数文件系统的默认 I/O 操作都是缓存 I/O。在 Linux 的缓存 I/O 机制中，操作系统会将 I/O 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。 缓存 I/O 的缺点：数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作，这些数据拷贝操作所带来的 CPU 以及内存开销是非常大的。 刚才说了，对于一次 IO 访问（以 read 举例），数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。所以说，当一个 read 操作发生时，它会经历两个阶段： 等待数据准备 (Waiting for the data to be ready) 将数据从内核拷贝到进程中 (Copying the data from the kernel to the process) 正式因为这两个阶段，linux 系统产生了下面五种网络模式的方案。 阻塞 I/O（blocking IO） 非阻塞 I/O（nonblocking IO） I/O 多路复用（ IO multiplexing） 信号驱动 I/O（ signal driven IO） 异步 I/O（asynchronous IO） 注：由于 signal driven IO 在实际中并不常用，所以我这只提及剩下的四种 IO Model。 阻塞 I/O（blocking IO）在 linux 中，默认情况下所有的 socket 都是 blocking，一个典型的读操作流程大概是这样： 当用户进程调用了 recvfrom 这个系统调用，kernel 就开始了 IO 的第一个阶段：准备数据（对于网络 IO 来说，很多时候数据在一开始还没有到达。比如，还没有收到一个完整的 UDP 包。这个时候 kernel 就要等待足够的数据到来）。这个过程需要等待，也就是说数据被拷贝到操作系统内核的缓冲区中是需要一个过程的。而在用户进程这边，整个进程会被阻塞（当然，是进程自己选择的阻塞）。当 kernel 一直等到数据准备好了，它就会将数据从 kernel 中拷贝到用户内存，然后 kernel 返回结果，用户进程才解除 block 的状态，重新运行起来。 所以，blocking IO 的特点就是在 IO 执行的两个阶段都被 block 了。 非阻塞 I/O（nonblocking IO）linux 下，可以通过设置 socket 使其变为 non-blocking。当对一个 non-blocking socket 执行读操作时，流程是这个样子： 当用户进程发出 read 操作时，如果 kernel 中的数据还没有准备好，那么它并不会 block 用户进程，而是立刻返回一个 error。从用户进程角度讲 ，它发起一个 read 操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个 error 时，它就知道数据还没有准备好，于是它可以再次发送 read 操作。一旦 kernel 中的数据准备好了，并且又再次收到了用户进程的 system call，那么它马上就将数据拷贝到了用户内存，然后返回。 所以，nonblocking IO 的特点是用户进程需要不断的主动询问 kernel 数据好了没有。 I/O 多路复用（ IO multiplexing）IO multiplexing 就是我们说的 select，poll，epoll，有些地方也称这种 IO 方式为 event driven IO。select/epoll 的好处就在于单个 process 就可以同时处理多个网络连接的 IO。它的基本原理就是 select，poll，epoll 这个 function 会不断的轮询所负责的所有 socket，当某个 socket 有数据到达了，就通知用户进程。 当用户进程调用了select，那么整个进程会被block，而同时，kernel 会 “监视” 所有 select 负责的 socket，当任何一个 socket 中的数据准备好了，select 就会返回。这个时候用户进程再调用 read 操作，将数据从 kernel 拷贝到用户进程。 所以，I/O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select() 函数就可以返回。 这个图和 blocking IO 的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个 system call (select 和 recvfrom)，而 blocking IO 只调用了一个 system call (recvfrom)。但是，用 select 的优势在于它可以同时处理多个 connection。 所以，如果处理的连接数不是很高的话，使用 select/epoll 的 web server 不一定比使用 multi-threading + blocking IO 的 web server 性能更好，可能延迟还更大。select/epoll 的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。） 在 IO multiplexing Model 中，实际中，对于每一个 socket，一般都设置成为 non-blocking，但是，如上图所示，整个用户的 process 其实是一直被 block 的。只不过 process 是被 select 这个函数 block，而不是被 socket IO 给 block。 异步 I/O（asynchronous IO）inux 下的 asynchronous IO 其实用得很少。先看一下它的流程： 用户进程发起 read 操作之后，立刻就可以开始去做其它的事。而另一方面，从 kernel 的角度，当它受到一个 asynchronous read 之后，首先它会立刻返回，所以不会对用户进程产生任何 block。然后，kernel 会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel 会给用户进程发送一个 signal，告诉它 read 操作完成了。 总结blocking 和 non-blocking 的区别调用 blocking IO 会一直 block 住对应的进程直到操作完成，而 non-blocking IO 在 kernel 还准备数据的情况下会立刻返回。 synchronous IO 和 asynchronous IO 的区别在说明 synchronous IO 和 asynchronous IO 的区别之前，需要先给出两者的定义。POSIX 的定义是这样子的： A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes; An asynchronous I/O operation does not cause the requesting process to be blocked; 两者的区别就在于 synchronous IO 做”IO operation” 的时候会将 process 阻塞。按照这个定义，之前所述的 blocking IO，non-blocking IO，IO multiplexing 都属于 synchronous IO。 有人会说，non-blocking IO 并没有被 block 啊。这里有个非常 “狡猾” 的地方，定义中所指的”IO operation”是指真实的 IO 操作，就是例子中的 recvfrom 这个 system call。non-blocking IO 在执行 recvfrom 这个 system call 的时候，如果 kernel 的数据没有准备好，这时候不会 block 进程。但是，当 kernel 中数据准备好的时候，recvfrom 会将数据从 kernel 拷贝到用户内存中，这个时候进程是被 block 了，在这段时间内，进程是被 block 的。 而 asynchronous IO 则不一样，当进程发起 IO 操作之后，就直接返回再也不理睬了，直到 kernel 发送一个信号，告诉进程说 IO 完成。在这整个过程中，进程完全没有被 block。 各个 IO Model 的比较如图所示： 通过上面的图片，可以发现 non-blocking IO 和 asynchronous IO 的区别还是很明显的。在 non-blocking IO 中，虽然进程大部分时间都不会被 block，但是它仍然要求进程去主动的 check，并且当数据准备完成以后，也需要进程主动的再次调用 recvfrom 来将数据拷贝到用户内存。而 asynchronous IO 则完全不同。它就像是用户进程将整个 IO 操作交给了他人（kernel）完成，然后他人做完后发信号通知。在此期间，用户进程不需要去检查 IO 操作的状态，也不需要主动的去拷贝数据。 select，poll，epoll 都是 IO 多路复用的机制。I/O 多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但 select，poll，epoll 本质上都是同步 I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步 I/O 则无需自己负责进行读写，异步 I/O 的实现会负责把数据从内核拷贝到用户空间。（这里啰嗦下） select1int select (int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); select 函数监视的文件描述符分 3 类，分别是 writefds、readfds、和 exceptfds。调用后 select 函数会阻塞，直到有描述副就绪（有数据 可读、可写、或者有 except），或者超时（timeout 指定等待时间，如果立即返回设为 null 即可），函数返回。当 select 函数返回后，可以 通过遍历 fdset，来找到就绪的描述符。 select 目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点。select 的一 个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在 Linux 上一般为 1024，可以通过修改宏定义甚至重新编译内核的方式提升这一限制，但 是这样也会造成效率的降低。 poll1int poll (struct pollfd *fds, unsigned int nfds, int timeout); 不同与 select 使用三个位图来表示三个 fdset 的方式，poll 使用一个 pollfd 的指针实现。 12345struct pollfd { int fd; short events; short revents; }; pollfd 结构包含了要监视的 event 和发生的 event，不再使用 select“参数 - 值” 传递的方式。同时，pollfd 并没有最大数量限制（但是数量过大后性能也是会下降）。 和 select 函数一样，poll 返回后，需要轮询 pollfd 来获取就绪的描述符。 从上面看，select 和 poll 都需要在返回后，通过遍历文件描述符来获取已经就绪的socket。事实上，同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降。 epollepoll 是在 2.6 内核中提出的，是之前的 select 和 poll 的增强版本。相对于 select 和 poll 来说，epoll 更加灵活，没有描述符限制。epoll 使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的 copy 只需一次。 一 epoll 操作过程epoll 操作过程需要三个接口，分别如下： 123int epoll_create(int size)；int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout); 1. int epoll_create(int size);创建一个 epoll 的句柄，size 用来告诉内核这个监听的数目一共有多大，这个参数不同于 select() 中的第一个参数，给出最大监听的 fd+1 的值，参数size并不是限制了epoll所能监听的描述符最大个数，只是对内核初始分配内部数据结构的一个建议。当创建好 epoll 句柄后，它就会占用一个 fd 值，在 linux 下如果查看 / proc / 进程 id/fd/，是能够看到这个 fd 的，所以在使用完 epoll 后，必须调用 close() 关闭，否则可能导致 fd 被耗尽。 *2. int epoll_ctl(int epfd, int op, int fd, struct epoll_event event)；函数是对指定描述符 fd 执行 op 操作。 epfd：是 epoll_create() 的返回值。 op：表示 op 操作，用三个宏来表示：添加 EPOLL_CTL_ADD，删除 EPOLL_CTL_DEL，修改 EPOLL_CTL_MOD。分别添加、删除和修改对 fd 的监听事件。 fd：是需要监听的 fd（文件描述符） epoll_event：是告诉内核需要监听什么事，struct epoll_event 结构如下： 12345678910111213struct epoll_event { __uint32_t events; epoll_data_t data; };EPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；EPOLLOUT：表示对应的文件描述符可以写；EPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；EPOLLERR：表示对应的文件描述符发生错误；EPOLLHUP：表示对应的文件描述符被挂断；EPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里 3. int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);等待 epfd 上的 io 事件，最多返回 maxevents 个事件。参数 events 用来从内核得到事件的集合，maxevents 告之内核这个 events 有多大，这个 maxevents 的值不能大于创建 epoll_create() 时的 size，参数 timeout 是超时时间（毫秒，0 会立即返回，-1 将不确定，也有说法说是永久阻塞）。该函数返回需要处理的事件数目，如返回 0 表示已超时。 二 工作模式 epoll 对文件描述符的操作有两种模式：LT（level trigger）和 ET（edge trigger）。LT 模式是默认模式，LT 模式与 ET 模式的区别如下： LT 模式：当 epoll_wait 检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用 epoll_wait 时，会再次响应应用程序并通知此事件。 ET 模式：当 epoll_wait 检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用 epoll_wait 时，不会再次响应应用程序并通知此事件。 1. LT 模式LT(level triggered) 是缺省的工作方式，并且同时支持 block 和 no-block socket. 在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的 fd 进行 IO 操作。如果你不作任何操作，内核还是会继续通知你的。 2. ET 模式ET(edge-triggered) 是高速工作方式，只支持 no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过 epoll 告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了 (比如，你在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致了一个 EWOULDBLOCK 错误）。但是请注意，如果一直不对这个 fd 作 IO 操作 (从而导致它再次变成未就绪)，内核不会发送更多的通知 (only once) ET 模式在很大程度上减少了 epoll 事件被重复触发的次数，因此效率要比 LT 模式高。epoll 工作在 ET 模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读 / 阻塞写操作把处理多个文件描述符的任务饿死。 3. 总结假如有这样一个例子： 我们已经把一个用来从管道中读取数据的文件句柄 (RFD) 添加到 epoll 描述符 这个时候从管道的另一端被写入了 2KB 的数据 调用 epoll_wait(2)，并且它会返回 RFD，说明它已经准备好读取操作 然后我们读取了 1KB 的数据 调用 epoll_wait(2)…… LT 模式：如果是 LT 模式，那么在第 5 步调用 epoll_wait(2) 之后，仍然能受到通知。 ET 模式：如果我们在第 1 步将 RFD 添加到 epoll 描述符的时候使用了 EPOLLET 标志，那么在第 5 步调用 epoll_wait(2) 之后将有可能会挂起，因为剩余的数据还存在于文件的输入缓冲区内，而且数据发出端还在等待一个针对已经发出数据的反馈信息。只有在监视的文件句柄上发生了某个事件的时候 ET 工作模式才会汇报事件。因此在第 5 步的时候，调用者可能会放弃等待仍在存在于文件输入缓冲区内的剩余数据。 当使用 epoll 的 ET 模型来工作时，当产生了一个 EPOLLIN 事件后，读数据的时候需要考虑的是当 recv() 返回的大小如果等于请求的大小，那么很有可能是缓冲区还有数据未读完，也意味着该次事件还没有处理完，所以还需要再次读取： 1234567891011121314151617181920212223while(rs){ buflen = recv(activeevents[i].data.fd, buf, sizeof(buf), 0); if(buflen &lt; 0){ if(errno == EAGAIN){ break; } else{ return; } } else if(buflen == 0){ } if(buflen == sizeof(buf){ rs = 1; } else{ rs = 0; }} Linux 中的 EAGAIN 含义 Linux 环境下开发经常会碰到很多错误 (设置 errno)，其中 EAGAIN 是其中比较常见的一个错误 (比如用在非阻塞操作中)。从字面上来看，是提示再试一次。这个错误经常出现在当应用程序进行一些非阻塞 (non-blocking) 操作 (对文件或 socket) 的时候。 例如，以 O_NONBLOCK 的标志打开文件 / socket/FIFO，如果你连续做 read 操作而没有数据可读。此时程序不会阻塞起来等待数据准备就绪返回，read 函数会返回一个错误 EAGAIN，提示你的应用程序现在没有数据可读请稍后再试。又例如，当一个系统调用 (比如 fork) 因为没有足够的资源 (比如虚拟内存) 而执行失败，返回 EAGAIN 提示其再调用一次(也许下次就能成功)。 三 代码演示下面是一段不完整的代码且格式不对，意在表述上面的过程，去掉了一些模板代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116#define IPADDRESS &quot;127.0.0.1&quot;#define PORT 8787#define MAXSIZE 1024#define LISTENQ 5#define FDSIZE 1000#define EPOLLEVENTS 100listenfd = socket_bind(IPADDRESS,PORT);struct epoll_event events[EPOLLEVENTS];epollfd = epoll_create(FDSIZE);add_event(epollfd,listenfd,EPOLLIN);for ( ; ; ){ ret = epoll_wait(epollfd,events,EPOLLEVENTS,-1); handle_events(epollfd,events,ret,listenfd,buf);}static void handle_events(int epollfd,struct epoll_event *events,int num,int listenfd,char *buf){ int i; int fd; for (i = 0;i &lt; num;i++) { fd = events[i].data.fd; if ((fd == listenfd) &amp;&amp;(events[i].events &amp; EPOLLIN)) handle_accpet(epollfd,listenfd); else if (events[i].events &amp; EPOLLIN) do_read(epollfd,fd,buf); else if (events[i].events &amp; EPOLLOUT) do_write(epollfd,fd,buf); }}static void add_event(int epollfd,int fd,int state){ struct epoll_event ev; ev.events = state; ev.data.fd = fd; epoll_ctl(epollfd,EPOLL_CTL_ADD,fd,&amp;ev);}static void handle_accpet(int epollfd,int listenfd){ int clifd; struct sockaddr_in cliaddr; socklen_t cliaddrlen; clifd = accept(listenfd,(struct sockaddr*)&amp;cliaddr,&amp;cliaddrlen); if (clifd == -1) perror(&quot;accpet error:&quot;); else { printf(&quot;accept a new client: %s:%d\\n&quot;,inet_ntoa(cliaddr.sin_addr),cliaddr.sin_port); add_event(epollfd,clifd,EPOLLIN); } }static void do_read(int epollfd,int fd,char *buf){ int nread; nread = read(fd,buf,MAXSIZE); if (nread == -1) { perror(&quot;read error:&quot;); close(fd); delete_event(epollfd,fd,EPOLLIN); } else if (nread == 0) { fprintf(stderr,&quot;client close.\\n&quot;); close(fd); delete_event(epollfd,fd,EPOLLIN); } else { printf(&quot;read message is : %s&quot;,buf); modify_event(epollfd,fd,EPOLLOUT); } }static void do_write(int epollfd,int fd,char *buf) { int nwrite; nwrite = write(fd,buf,strlen(buf)); if (nwrite == -1){ perror(&quot;write error:&quot;); close(fd); delete_event(epollfd,fd,EPOLLOUT); }else{ modify_event(epollfd,fd,EPOLLIN); } memset(buf,0,MAXSIZE); }static void delete_event(int epollfd,int fd,int state) { struct epoll_event ev; ev.events = state; ev.data.fd = fd; epoll_ctl(epollfd,EPOLL_CTL_DEL,fd,&amp;ev);}static void modify_event(int epollfd,int fd,int state){ struct epoll_event ev; ev.events = state; ev.data.fd = fd; epoll_ctl(epollfd,EPOLL_CTL_MOD,fd,&amp;ev);} 四 epoll 总结在 select/poll 中，进程只有在调用一定的方法后，内核才对所有监视的文件描述符进行扫描，而 epoll 事先通过 epoll_ctl() 来注册一 个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似 callback 的回调机制，迅速激活这个文件描述符，当进程调用 epoll_wait() 时便得到通知。(此处去掉了遍历文件描述符，而是通过监听回调的的机制。这正是 epoll 的魅力所在。) epoll 的优点主要是一下几个方面： 监视的描述符数量不受限制，它所支持的 FD 上限是最大可以打开文件的数目，这个数字一般远大于 2048, 举个例子, 在 1GB 内存的机器上大约是 10 万左 右，具体数目可以 cat /proc/sys/fs/file-max 察看, 一般来说这个数目和系统内存关系很大。select 的最大缺点就是进程打开的 fd 是有数量限制的。这对 于连接数量比较大的服务器来说根本不能满足。虽然也可以选择多进程的解决方案 (Apache 就是这样实现的)，不过虽然 linux 上面创建进程的代价比较小，但仍旧是不可忽视的，加上进程间数据同步远比不上线程间同步的高效，所以也不是一种完美的方案。 IO 的效率不会随着监视 fd 的数量的增长而下降。epoll 不同于 select 和 poll 轮询的方式，而是通过每个 fd 定义的回调函数来实现的。只有就绪的 fd 才会执行回调函数。 如果没有大量的 idle -connection 或者 dead-connection，epoll 的效率并不会比 select/poll 高很多，但是当遇到大量的 idle- connection，就会发现 epoll 的效率大大高于 select/poll。 用户空间与内核空间，进程上下文与中断上下文 [总结]进程切换维基百科 - 文件描述符Linux 中直接 I/O 机制的介绍IO - 同步，异步，阻塞，非阻塞 （亡羊补牢篇）Linux 中 select poll 和 epoll 的区别IO 多路复用之 select 总结IO 多路复用之 poll 总结IO 多路复用之 epoll 总结","link":"/2020/03/25/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91Linux-IO-%E6%A8%A1%E5%BC%8F%E5%8F%8A-select%E3%80%81poll%E3%80%81epoll-%E8%AF%A6%E8%A7%A3/"}],"tags":[{"name":"CIDR","slug":"CIDR","link":"/tags/CIDR/"},{"name":"子网","slug":"子网","link":"/tags/%E5%AD%90%E7%BD%91/"},{"name":"IPv4","slug":"IPv4","link":"/tags/IPv4/"},{"name":"SSH","slug":"SSH","link":"/tags/SSH/"},{"name":"娱乐","slug":"娱乐","link":"/tags/%E5%A8%B1%E4%B9%90/"},{"name":"windows","slug":"windows","link":"/tags/windows/"},{"name":"键盘","slug":"键盘","link":"/tags/%E9%94%AE%E7%9B%98/"},{"name":"可伸缩性","slug":"可伸缩性","link":"/tags/%E5%8F%AF%E4%BC%B8%E7%BC%A9%E6%80%A7/"},{"name":"可扩展性","slug":"可扩展性","link":"/tags/%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7/"},{"name":"工具","slug":"工具","link":"/tags/%E5%B7%A5%E5%85%B7/"},{"name":"ssh","slug":"ssh","link":"/tags/ssh/"},{"name":"shadowsocks","slug":"shadowsocks","link":"/tags/shadowsocks/"},{"name":"iptables","slug":"iptables","link":"/tags/iptables/"},{"name":"kubernetes","slug":"kubernetes","link":"/tags/kubernetes/"},{"name":"容器","slug":"容器","link":"/tags/%E5%AE%B9%E5%99%A8/"},{"name":"虚拟化","slug":"虚拟化","link":"/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"内核","slug":"内核","link":"/tags/%E5%86%85%E6%A0%B8/"},{"name":"心情","slug":"心情","link":"/tags/%E5%BF%83%E6%83%85/"}],"categories":[{"name":"计算机网络","slug":"计算机网络","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"Linux运维","slug":"Linux运维","link":"/categories/Linux%E8%BF%90%E7%BB%B4/"},{"name":"win系列","slug":"win系列","link":"/categories/win%E7%B3%BB%E5%88%97/"},{"name":"架构","slug":"架构","link":"/categories/%E6%9E%B6%E6%9E%84/"},{"name":"日记本","slug":"日记本","link":"/categories/%E6%97%A5%E8%AE%B0%E6%9C%AC/"}],"pages":[{"title":"关于我","text":"2023.07.10 别回头，向前看！ 主线任务更新： 有一个喜欢的女孩子 学会驾车，和她一起去旅游，拍照 有固定居所，一只边牧，一只猫","link":"/about/index.html"}]}